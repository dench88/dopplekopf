import gymnasium as gymfrom gymnasium import spacesimport constantsfrom game_state import GameStateimport numpy as npimport randomfrom agents import ExpectiMaxAgent, RandomAgentimport constantsTYPES = constants.typesSUITS = constants.suitsCARD_TYPES = [(r, s) for r in TYPES for s in SUITS]TYPE_TO_IDX = {t: i for i, t in enumerate(CARD_TYPES)}class DoppelkopfEnv(gym.Env):    def __init__(self, player_name: str, expectimax_prob: float = 0.85):        super().__init__()        self.player = player_name        self.agent = None        self.opponent_agents = {}        self.expectimax_prob = expectimax_prob        self._assign_opponents()        hand_size = len(CARD_TYPES)        self.action_space = spaces.Discrete(hand_size)        self.hand_offset = 0        self.seen_offset = hand_size        self.points_offset = hand_size + hand_size        self.trick_offset = self.points_offset + len(constants.players)        prelim_obs_size = hand_size + hand_size + len(constants.players) + 1        self.team_flag_offset = self.trick_offset + 1        self.partner_offset = self.team_flag_offset + 1        n_other_players = len(constants.players) - 1        obs_size = prelim_obs_size + 1 + n_other_players        self.observation_space = spaces.Box(            low=0,            high=2,            shape=(obs_size,),            dtype=np.int16        )        self.state = None    def _assign_opponents(self):        self.opponent_agents.clear()        for p in constants.players:            if p == self.player:                continue            if random.random() < self.expectimax_prob:                self.opponent_agents[p] = ExpectiMaxAgent(p, verbose=False)            else:                self.opponent_agents[p] = RandomAgent()    def reset(self, *, seed=None, options=None):        if seed is not None:            import random as _random            _random.seed(seed)        self.state = GameState.random_deal()        if hasattr(self, "_assign_opponents"):            self._assign_opponents()        if getattr(self, "agent", None):            self.agent.update_team_info(self.state, force=True)        obs = self._encode(self.state)        return obs, {}    def step(self, action):        # (1        legal = self.state.legal_actions()        card = self._decode_action(action, legal)        state_after_me = self.state.apply_action(card)        for opp_name, opp_agent in self.opponent_agents.items():            opp_card = opp_agent.choose(state_after_me)            state_after_me = state_after_me.apply_action(opp_card)        # (3) Reward        reward = self._compute_reward(self.state, state_after_me)        # (4) Update state        self.state = state_after_me        # (5) Team flags        if hasattr(self, "agent") and self.agent:            self.agent.update_team_info(self.state)        done = self.state.is_terminal()        obs = self._encode(self.state)        return obs, reward, done, False, {}    def render(self, mode="human"):        print(self.state)    def _encode(self, state: GameState):        vec = np.zeros(self.observation_space.shape[0], dtype=np.int16)        for c in state.hands[self.player]:            idx = TYPE_TO_IDX[(c.type, c.suit)]            vec[self.hand_offset + idx] += 1        for trick in (*state.trick_history, state.current_trick):            for _, card in trick:                idx = TYPE_TO_IDX[(card.type, card.suit)]                vec[self.seen_offset + idx] += 1        for i, p in enumerate(constants.players):            vec[self.points_offset + i] = state.points[p]        vec[self.trick_offset] = len(state.trick_history)        if getattr(self, "agent", None):            vec[self.team_flag_offset] = int(self.agent.is_team_playing)        else:            vec[self.team_flag_offset] = 0        if getattr(self, "agent", None):            partner_list = self.agent.get_team_members(state)            others = [p for p in constants.players if p != self.player]            for i, p in enumerate(others):                vec[self.partner_offset + i] = 1 if p in partner_list else 0        return vec    def _decode_action(self, idx, legal):        for c in legal:            if TYPE_TO_IDX[(c.type, c.suit)] == idx:                return c        return random.choice(legal)    def _compute_reward(self, old: GameState, new: GameState) -> float:        if not new.is_terminal():            if len(new.trick_history) > len(old.trick_history):                last_trick = new.trick_history[-1]                trick_pts = sum(card.points for _, card in last_trick)                suit = last_trick[0][1].category                def strength(pair):                    _, card = pair                    return card.power if card.category in (suit, "trumps") else -1                winner = max(last_trick, key=strength)[0]                if getattr(self, "agent", None):                    team = self.agent.get_team_members(new)                else:                    team = []                return trick_pts if (winner in team) else -trick_pts            return 0        if getattr(self, "agent", None):            self.agent.update_team_info(new, force=True)            team = self.agent.get_team_members(new)            team_pts = sum(new.points[p] for p in team)            opp_pts = sum(v for p, v in new.points.items() if p not in team)            return (team_pts - opp_pts) if (self.player in team) else (opp_pts - team_pts)        else:            my_pts = new.points[self.player]            opp_pts = sum(v for p, v in new.points.items() if p != self.player)            return my_pts - opp_pts